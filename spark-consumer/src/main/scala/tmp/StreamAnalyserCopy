package com.redmart.spark.kafka.integ

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.Seconds
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.StreamingContext._
import org.apache.spark.streaming.kafka.KafkaUtils
import org.json4s._
import org.json4s.native.JsonMethods._
import com.datastax.spark.connector._
import com.datastax.spark.connector.streaming._
import java.util.Properties

/**
 * Consumes messages from one or more topics in Kafka 
 * and calculates pick rate per user basis.
 */

object StreamAnalyser {

  implicit val formats = DefaultFormats

  def parser(json: String): Event = {
    val parsedJson = parse(json)
    return parsedJson.extract[Event] // Extract into the case class
  }

  def main(args: Array[String]) = {
    printf("--------------- starting spark consumer -----------------");

    val zkQuorum = System.getenv("ZOOKEEPER_HOST"); //args(0);
    val group = "spark_cluster";//args(1);
    val topics = "page_visits";
    val numThreads = 2;
    
    val cassandraHosts = System.getenv("CASSANDRA_HOST_0") + "," + System.getenv("CASSANDRA_HOST_1")
 
    val sparkConf = new SparkConf().setAppName("Pick-Rate-Analyser")
      .set("spark.cassandra.connection.host", cassandraHosts)
      .set("cassandra.connection.native.port", "9042")

    val ssc = new StreamingContext(sparkConf, Seconds(10))

    ssc.checkpoint("checkpoint")
    val topicMap = topics.split(",").map((_, numThreads.toInt)).toMap
    val kafkaDStream = KafkaUtils.createStream(ssc, zkQuorum, group, topicMap, StorageLevel.MEMORY_ONLY)
    //kafkaDStream.print()

    val rdds = kafkaDStream.map(_._2).map(parser)
    rdds.persist()
    rdds.saveToCassandra("redmart", "pick_events", SomeColumns("time_stamp", "user_id", "session_id", "event_type", "item", "order_id", "tote_id", "quantity", "from_location"))
    
    // rdds.saveToCassendra event table.

    val logginStream = rdds.filter(s => s.eventType == "LoggedIn") // filter loggedIn events and update the latest loggedIn time
      .map(a => (a.userId, a))
      .updateStateByKey[Long](DataAnalyser.keepLoggedInTime) // return (user_id, latest_logged_time)

    
//    logoutStream.map(a => 
//      ssc.cassandraTable("redmart", "pick_events").where("userId", a._1)
//    )
    
      
    val pickStream = rdds.filter(s => s.eventType == "PickConfirm").map(s => (s.userId, s))  // filter only Pick stream and map into K,V
    val join = logginStream.join(pickStream) // (user_id, (login_time, event))
    

    val filteredStream = join.updateStateByKey[Seq[Event]](DataAnalyser.updateFunc) // return (user_id, seq[events]), keep the current event_counts, filter older events
    filteredStream.print()
    
    val eventCountStrem = filteredStream.map{case (user_id, eventSeq) => (user_id, eventSeq.length)} // get (user_id, event_count)
    
    val logoutStream = rdds.filter { s => s.eventType == "LoggedOut"}.map( x => (x.userId, x.timeStamp))
    
    
   // val filterredLogoutStream = filteredStream.join(logoutStream) // (user_id, (eventSeq, user))

    
    val currentStream = eventCountStrem.join(logginStream) // return (user_id, (event_count, login_time)
    currentStream.print()
    
    val joinedLogoutStream = currentStream.join(logoutStream); // return (user_id, ((event_count, login_time), logoutTime))
    
    val finalPickRateStream =  joinedLogoutStream.map{case (user_id, (t, logoutTime)) => (DataAnalyser.getUUID, user_id, DataAnalyser.getPickRate(t._1, t._2, logoutTime), logoutTime)} // return (user_id, pick_rate, date)
    finalPickRateStream.saveToCassandra("redmart", "pickrate_history", SomeColumns("uid", "user_id", "pick_rate", "time_stamp"));
    
    
    
    val finalStream = currentStream.map{case (user_id,(event_count,login_time)) => (user_id, DataAnalyser.getRate(event_count,login_time))} // return (user_id, pick_rate(per minute))


    finalStream.saveToCassandra("redmart", "user_pick_rate", SomeColumns("user_id", "pick_rate"))
    finalStream.print()
    
    
    // Sending the results back to a KAFKA topic
    // Zookeper connection properties
    val brokers = System.getenv("KAFKA_BROKERS")
    val props = new Properties()
    props.put("metadata.broker.list", brokers)
    props.put("serializer.class", "kafka.serializer.StringEncoder")
    

    ssc.start()
    ssc.awaitTermination()

    printf("----------------context is created RM -----------------------")

  }

  def sendToCassendra(events: List[Event]) = {
    for (event <- events) {
      println("userID : " + event.userId);
    }

  }

 

}